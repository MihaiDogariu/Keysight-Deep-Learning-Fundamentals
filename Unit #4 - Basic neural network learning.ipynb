{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmdlBnHChvyIXl1YIL7kzv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MihaiDogariu/Keysight-Deep-Learning-Fundamentals/blob/main/Unit%20%234%20-%20Basic%20neural%20network%20learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic neural network learning\n",
        "\n",
        "This notebook introduces the backpropagation algorithm on a simple task, namely classifying iris flowers based on some of their characteristics. We also point out the importance of the bias term in neural nets by leaving it out for this example and observing its influence on the final outcome.\n"
      ],
      "metadata": {
        "id": "ew31qoz2SiNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "3r7MKm0XTp5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we need to load the data. The Iris dataset is a very popular one for machine learning classifications. It contains 150 entries, corresponding to 3 classes. Each of the 3 iris types has 50 occurences in the dataset. For each sample we have 4 features available: sepal length, sepal width, petal length, petal width."
      ],
      "metadata": {
        "id": "G5RZlJiL8kJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the \"Iris\" dataset: https://archive.ics.uci.edu/ml/datasets/iris\n",
        "data = load_iris()\n",
        "# Extract data and labels from the dataset\n",
        "X=data.data\n",
        "y=data.target\n",
        "# Split the data in a 80-20 ratio for train-test, respectively\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=20, random_state=4)"
      ],
      "metadata": {
        "id": "Iv-L3S6bTtRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the hyperparameters\n",
        "learning_rate = 0.1\n",
        "iterations = 5000\n",
        "N = y_train.size\n",
        "\n",
        "# Setting the network's dimensions\n",
        "input_size = 4 # corresponding to the 4 input features\n",
        "hidden_size = 3\n",
        "output_size = 3 # corresponding to the 3 target classes\n",
        "\n",
        "# We will store the results in a pd.DataFrame for easier post-processing\n",
        "results = pd.DataFrame(columns=[\"mse\", \"accuracy\"])"
      ],
      "metadata": {
        "id": "gTHOWhTYT0Cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to setup the weights of a 2 layer network."
      ],
      "metadata": {
        "id": "9p4NvtFjBRGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the weights between the first and the hidden layer\n",
        "W1 = np.random.normal(scale=0.5, size=(input_size, hidden_size))\n",
        "\n",
        "# Initialize the weights between the hidden and the output layer\n",
        "W2 = np.random.normal(scale=0.5, size=(hidden_size , output_size))"
      ],
      "metadata": {
        "id": "YUbIuB1UTwVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to define ourselves the functions that we will use: the activation, the cost function, the way the metric (accuracy) will be computed and the transformation to one-hot encoded vectors."
      ],
      "metadata": {
        "id": "VgNlClUfokLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the sigmoid function\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Defining the Mean Squared Error\n",
        "def mean_squared_error(y_pred, y_true):\n",
        "  return ((y_pred - y_true)**2).sum() / (2*y_pred.size)\n",
        "\n",
        "# Defining accuracy\n",
        "def accuracy(y_pred, y_true):\n",
        "  acc = y_pred.argmax(axis=1) == y_true.argmax(axis=1)\n",
        "  return acc.mean()\n",
        "\n",
        "# Defining one-hot encoding\n",
        "def one_hot(x):\n",
        "  result = np.zeros((x.size, x.max()+1))\n",
        "  result[np.arange(x.size), x] = 1\n",
        "  return result"
      ],
      "metadata": {
        "id": "iRrrdqXpUA-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model."
      ],
      "metadata": {
        "id": "l0R79MUXoz4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the labels into one-hot encoded vectors\n",
        "one_hot_y_train = one_hot(y_train)\n",
        "one_hot_y_test = one_hot(y_test)\n",
        "\n",
        "mse_list = []\n",
        "acc_list = []\n",
        "\n",
        "for itr in range(iterations):\n",
        "  # Forward propagate the input through the first layer\n",
        "  A1 = sigmoid(np.dot(X_train, W1))\n",
        "\n",
        "  # Forward propagate the output of the first layer towards the output\n",
        "  A2 = sigmoid(np.dot(A1, W2))\n",
        "\n",
        "  # Compute the cost function (mse) and the metric (acc)\n",
        "  mse = mean_squared_error(A2, one_hot_y_train)\n",
        "  acc = accuracy(A2, one_hot_y_train)\n",
        "\n",
        "  # Save both mse and acc\n",
        "  mse_list.append(mse)\n",
        "  acc_list.append(acc)\n",
        "\n",
        "  # Backpropagation\n",
        "  # Compute the gradient for the weights between the output and the hidden layer\n",
        "  E1 = A2 - one_hot_y_train\n",
        "  dW1 = E1 * A2 * (1 - A2)\n",
        "\n",
        "  # Compute the gradient for the weights between the hidden and the input layer\n",
        "  E2 = np.dot(dW1, W2.T)\n",
        "  dW2 = E2 * A1 * (1 - A1)\n",
        "\n",
        "  # Update the weights\n",
        "  W2_update = np.dot(A1.T, dW1) / N\n",
        "  W1_update = np.dot(X_train.T, dW2) / N\n",
        "\n",
        "  W2 = W2 - learning_rate * W2_update\n",
        "  W1 = W1 - learning_rate * W1_update"
      ],
      "metadata": {
        "id": "GRfddHzcSoiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the model by running forward propagation on the test dataset."
      ],
      "metadata": {
        "id": "ELVi87JooyUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rulam modelul antrenat pe baza de date de test\n",
        "Z1 = np.dot(X_test, W1)\n",
        "A1 = sigmoid(Z1)\n",
        "\n",
        "Z2 = np.dot(A1, W2)\n",
        "A2 = sigmoid(Z2)\n",
        "\n",
        "acc = accuracy(A2, one_hot_y_test)\n",
        "plt.plot(np.arange(iterations), acc_list)\n",
        "plt.show()\n",
        "print(\"Accuracy: {}\".format(acc))"
      ],
      "metadata": {
        "id": "KLXugNjbozL5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}