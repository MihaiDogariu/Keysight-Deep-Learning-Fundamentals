{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMA99ZMttduLr0GnyogxW2M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MihaiDogariu/Keysight-Deep-Learning-Fundamentals/blob/main/Unit%20%233%20-%20Gradient%20descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient descent\n",
        "This notebook makes a short introduction of the gradient descent algorithm. It shows the steps that the gradient descent algorithm takes in order to find the minimum of a cost function."
      ],
      "metadata": {
        "id": "5wH9x35AKdkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "63vyVnDtMQ0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start by defining both the cost function and its derivative. For practical reasons, $f(x)=x^2$ is a good pick."
      ],
      "metadata": {
        "id": "r4Unhf9jpKDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the cost function\n",
        "def J(x):\n",
        "\treturn x**2.0\n",
        "\n",
        "# Defining the derivative function\n",
        "def dJ(x):\n",
        "\treturn x * 2.0"
      ],
      "metadata": {
        "id": "YFAFBeEmNaHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the cost function on a fixed support, centered around 0."
      ],
      "metadata": {
        "id": "lSHGX43HqqSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the cost function support\n",
        "support_fct = np.asarray([[-2.0, 2.0]])\n",
        "# Sample values from inside the function support with a step of 0.1\n",
        "f_input = np.arange(support_fct[0,0], support_fct[0,1]+0.1, 0.1)\n",
        "# Compute the function's output in the sampled points\n",
        "f_output = J(f_input)\n",
        "\n",
        "# Plot the function's values vs its support\n",
        "plt.plot(f_input, f_output)\n",
        "plt.xlabel(\"w\")\n",
        "plt.ylabel(\"J(w)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "floWehpmUuxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose the number of times that we run the gradient descent\n",
        "n_iter = 10\n",
        "# Choose the learning rate\n",
        "learning_rate = 0.09\n",
        "\n",
        "# Create 2 lists in which we will retain all values of w and J(w) that the gradient descent passes through\n",
        "list_w, list_J_w = [], []\n",
        "# Sample a random point on the function's support\n",
        "w = np.random.uniform(support_fct[0, 0], support_fct[0, 1])\n",
        "print (\"Starting algorithm from value {}.\".format(w))\n",
        "\n",
        "# Save the starting values of w and J(w)\n",
        "list_w.append(w)\n",
        "list_J_w.append(J(w))\n",
        "\n",
        "for i in range(n_iter):\n",
        "\t# Compute the gradient\n",
        "\tgradient = dJ(w)\n",
        "\t# Update w\n",
        "\tw = w - learning_rate * gradient\n",
        "\t# Compute J(w) in the updated w\n",
        "\tsolution_eval = J(w)\n",
        "\t# Save the new values of w and J(w)\n",
        "\tlist_w.append(w)\n",
        "\tlist_J_w.append(solution_eval)\n",
        "\t# Afisam pe ecran noile valori w si J(w)\n",
        "\tprint('>%d f(%s) = %.5f' % (i, w, solution_eval))\n",
        "\n",
        "\n",
        "# Display the initial cost function\n",
        "plt.plot(f_input, f_output)\n",
        "# Plot the memorized w and J(w) values\n",
        "plt.plot(list_w, list_J_w, '.-', color='red')\n",
        "plt.xlabel(\"w\")\n",
        "plt.ylabel(\"J(w)\")\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v8wHTBrdKvC7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}